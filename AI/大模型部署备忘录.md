# 大模型部署备忘录

## open-webui
- 简介  
  ollama客户端

- docker 

	```shell
	docker run -d -p 3010:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main
	```

- 账号  
	用户名:813102522@qq.com; 密码:123456

- 链接  
  - [openwebui](https://docs.openwebui.com/)

## lobe-chat
- 简介  
  生成式AI客户端，可连接本地ollama和其它开源大模型
- docker 

	```shell
	docker run -d -p 3210:3210 -e OPENAI_API_KEY=sk-xuan -e ACCESS_CODE=lobe66 --name lobe-chat  lobehub/lobe-chat
	```

## MaxKb
- 简介   
  本地知识库
- docker

	```shell
	docker run -d --name=maxkb -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data 1panel/maxkb
	```
- 链接
	- [Github MaxKB](https://github.com/1Panel-dev/MaxKB)

- 账号  
	用户名:Admin; 密码:Maxkb123Guxing


## 配置相关

- docker中若使用本机的ollama，需要配置为
  ```text
  http://host.docker.internal:11434
  ```
## draw.io

```docker
docker run -d --name="draw" -p 8081:8080 -p 8443:8443 fjudith/draw.io
```

## ollama知识

- 环境变量

使用 "ollama serve" 启动时，可以看到
```test
OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST: OLLAMA_KEEP_ALIVE:600 OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS: OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR:C:\\Users\\16336\\AppData\\Local\\Programs\\Ollama\\ollama_runners OLLAMA_TMPDIR:
```
这里面就是环境变量，可以自己设置到PC的用户变量中

|字段|默认值|含义|
|--|--|--|
|OLLAMA_DEBUG|false||
|OLLAMA_FLASH_ATTENTION|false||
|OLLAMA_HOST|||
|OLLAMA_KEEP_ALIVE|5m|控制模型在内存中的存活时间,默认为5分钟|
|OLLAMA_LLM_LIBRARY|||
|OLLAMA_MAX_LOADED_MODELS|1|限制了Ollama可以同时加载的模型数量。|
|OLLAMA_MAX_QUEUE|512||
|OLLAMA_MAX_VRAM|||
|OLLAMA_MODELS|||
|OLLAMA_NOHISTORY|false||
|OLLAMA_NOPRUNE|||
|OLLAMA_NUM_PARALLEL|1|可以同时处理的用户请求数量,若设置4，则可以同时处理两个用户的请求|
|OLLAMA_ORIGINS|||
|OLLAMA_RUNNERS_DIR|||
|OLLAMA_PORT|11434|Ollama的默认端口|
|CUDA_VISIBLE_DEVICES||指定使用哪个GPU，-1使用CPU|

- 其它参数设置

```shell

ollama list # 列车当前可用模型

ollama run [模型名称] # 对话模式运行一个模型

/set parameter num_ctx 4096 # 设置令牌大小，默认2048。此命令需要ollama run 之后执行

ollama run [模型名称] "[问题]" # 直接提问问题

ollama run [模型名称] "[问题]"  --verbose # 显示执行后的统计信息

```

- 模型命名
  - chat：模型专注于对话交互，能够在多轮对话中保持连贯性
  - text:模型适用于广泛的文本生成任务，能够根据输入的提示生成文本。
  - instruct:型强调的是理解和执行指令的能力，能够根据用户的指令生成高质量的文本。

## ollama docker方式

- 不使用GPU的方式

```shell
## 运行docker容器
docker run -d -v ollama:/root/.ollama -p 11435:11434 --name ollama-5 ollama/ollama
docker run -d -v ollama:/root/.ollama -p 11436:11434 --name ollama-6 ollama/ollama
docker run -d -v ollama:/root/.ollama -p 11437:11434 --name ollama-7 ollama/ollama
## 进入docker中的ollama
docker exec -it ollama-5 ollama [命令]
```

## 知识库相关资料

- [https://github.com/langgenius/dify](https://github.com/langgenius/dify)

- [https://github.com/sugarforever/chat-ollama](https://github.com/sugarforever/chat-ollama)

- [Reor](https://www.reorproject.org/)

- [mrdoc]() 

- [http://draw.io]()


## ollama资料

- [API参考](https://ollama.fan/reference/api/)
- [API 有效参数及其值](https://ollama.fan/reference/modelfile/#valid-parameters-and-values)
- [Ollama 可以设置的环境变量](https://blog.csdn.net/engchina/article/details/138743634)
- [如何与 Ollama 一起在本地运行大语言模型](https://zhuanlan.zhihu.com/p/690122036)
- [开源模型基础与能力解析——模型简介和参数解读](https://juejin.cn/post/7385752495534604303)
- [Ollama 本地运行大模型(LLM)完全指南](https://www.53ai.com/news-frame?id=5710.html)
- [Ollama 架构解析](https://blog.inoki.cc/2024/04/16/Ollama-cn/)
- [ollama应用全面解析：20个问题精通ollama](https://techdiylife.github.io/blog/blog.html?category1=c02&blogid=0037#ollama%E5%BA%94%E7%94%A8%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%9020%E4%B8%AA%E9%97%AE%E9%A2%98%E7%B2%BE%E9%80%9Aollama)

## LM Studio资料 

- [LM Studio Documentation](https://lmstudio.ai/docs/welcome)
- [本地部署大模型：LM Studio使用方法](https://zhuanlan.zhihu.com/p/692372977)
- [LM Studio语言大模型部署软件搜索语言模型报错“Error searching for models ‘Network error‘”解决办法
](https://blog.csdn.net/u012514495/article/details/139418711)
  >搜索目录“C:\Users\[用户]\AppData\Local\LM-Studio”，把所有“huggingface.co”都替换为“hf-mirror.com”

## Xinference

- [github 源码](https://github.com/xorbitsai/inference)


## 搜索引擎：Perplexica

- [Perplexica Github](https://github.com/ItzCrazyKns/Perplexica)

## 模型参数

|参数|建议值|含义|适用平台|
|--|--|--|--|
|Context Length|不同模型不一样|上下文大小||
|Temperature|0.8|控制模型随机性和多样性。</br>值越大，回答越随意||
|GPU Offload|根据模型设置|GPU占用比例|LM Studio|
|Flash Attention|true|一种提升Transformer模型效率的方法。</br>有助于缩短模型训练时间和推理延迟。|LM Stuido|


## 应用场景

- 实时翻译
- 英语课程
- 给定材料，生成练习题
- 以专有文档交流