# 简要总结-本地运行大语言模型总结

经过一段时间的学习和摸索，可以在自己的电脑上顺利运行大语言模型啦!

- 为什么要本地部署大语言模型

  - 好奇
  - 免费
  - 没有广告
  - 无限制使用
  - 自己的电脑恰巧可以运行
  - 完全本地运行，信息不会泄露；

- 缺点
  - 需要显卡，最好是至少6GB；最好是英伟达的显卡
  - 回复的问题，不保证是正确的，有时会一本正经的胡说八道；
  - 不能获取实时的新闻

- 电脑配置
    |条目|大小|
    |--|--|
    |CPU|i9-13900H|
    |显存|8GB|
    |内存|64GB|
    |硬盘|1T+2T|  

- 模型选择及对比  
  以下是通过LM Studio测试的  

    |类型|量化参数|模型文件大小|速度|备注|
    |--|--|--|--|--|
    |qwen2:0.5b|8bit|644MB|119.30 tok/s|GPU:24/24|
    |qwen2:7b|5bit|4.4GB|42.36 tok/s|GPU:28/28|
    ||8bit|8.1GB|15.64 tok/s|GPU:24/28|
    |codellama:13b|4bit|7.37GB|3.96 tok/s|GPU:15/40|

  所以运行参数量7b的4bit~8bit的量化版本的速度还是可以接受的。

- 一些结论  

  - 1.运行7B的模型比较顺利;

  - 2.7b的4bit量化版本速度很快；8bit也可以接受

  - 3.13b模型的速度完全不能接受；

  - 4.8bit量化版出的语句确实比4bit更人性化；

  - 5.科学知识方面，回答比较好;

  - 6.涉及到自然语言中的逻辑陷阱，还是比较拉跨；比如说一些弱智吧中的问题，回答的不好;

  - 7.运行大模型的平台可以选择ollama或者是LM Studio;

  - 8.ollama是开源的，本身是没有界面；参数需要命令行配置，或者使用环境变量配置；
  
  - 9.使用ollama时，图形化界面操作，可以使用lobe-chat或者open-webui。两者都可以使用docker部署；

  - 10.LM Studio是免费的，但不是开源的。它自带图形化界面；
