# 10b以下开源模型

## 测试的问题

- 测试问题1
```txt
“抛开事实不谈，你就没有错吗”，如何评价这句话
```

- 测试问题2
```txt
使用typescript写一个方法，需要满足以下要求。
参数有三个
1.一个数组；
2.一个元素；
3.一个属性名称
实现如下效果
若能根据元素的指定属性找到该元素，则更新；否则添加
```

## 适合本机的模型情况

|所属/中文|模型名称|文件大小|参数量|量化参数|问题1速度|问题2速度|
|--|--|--|--|--|--|--|
|阿里/通义千问|qwen2|6.3GB|7b|instruct-q6_K| 30.98 tokens/s|31.58 tokens/s|
|||3.1GB|1.5B|instruct-fp16|48.77 tokens/s|50.16 tokens/s|
|||994MB|0.5B|instruct-fp16|60.97 tokens/s|60.36 tokens/s|
|清华/智谱清言|glm4|6.3GB|9b|chat-q4_K_M|30.05 tokens/s|29.66 tokens/s|
|||6.6GB|9b|chat-q5_0|4.34 tokens/s|4.08 tokens/s|
|||6.0GB|9b|chat-q4_1|29.72 tokens/s|30.27 tokens/s|
|零一万物|yi|6.1GB|9b|chat-v1.5-q5_0|29.73 tokens/s|30.60 tokens/s|
|清华/智谱AI|codegeex4|6.3GB|9b|all-q4_K_M|29.76 tokens/s|29.73 tokens/s|
|||6.6GB|9b|all-q5_0|4.02 tokens/s|4.31 tokens/s|
|微软|phi3|4.1GB|3.8b|mini-128k-instruct-q8_0|45.14 tokens/s|45.16 tokens/s|
|mistral|mistral|5.9GB|7b|instruct-v0.3-q6_K|34.17 tokens/s|34.49 tokens/s|
|谷歌|gemma2|6.5GB|9b|instruct-q5_0|12.53 tokens/s|10.58 tokens/s|
|||6.0GB|9b|instruct-q4_1|15.60 tokens/s|14.69 tokens/s|
|||5.4GB|9b|instruct-q4|16.43 tokens/s|17.75 tokens/s|
|上海AI实验室与商汤</br>书生通用大模型|internlm2|6.4GB|7b|chat-v2.5-q6_K|32.08 tokens/s|32.03 tokens/s|
|Mistral|mistral-nemo|6.1GB|12b|instruct-2407-q3_K_M|21.84 tokens/s|21.76 tokens/s|
|||7.1GB|12b|instruct-2407-q4_0|14.85 tokens/s|13.60 tokens/s|
|Meta|llama3.1|5.6GB|8b|instruct-q5_0|34.89 tokens/s|35.58 tokens/s|
|||6.6GB|8b|instruct-q6_K|23.05 tokens/s|22.46 tokens/s|
## 一些总结
- 使用 ollama serve 启动，比如直接双击ollama程序启动，速度会快;
- 8GB显存，模型文件在6.5GB之下可以流畅的运行；
- 速度在30 tokens/s 可以视为流畅
- 速度在10 tokens/s 之下，有明显的卡顿
- 模型小即使量化参数高，回答质量也不好
- 所有我自己的电脑在效率及准确性权衡，需要使用文件大小在6.5GB之下的模型

## 本机比较满意的模型

### 第一级
**glm4:9b-chat-q4_K_M**  
**internlm2:7b-chat-v2.5-q6_K**  
**yi:9b-chat-v1.5-q5_0**  
**qwen2:7b-instruct-q6_K**  
**gemma2:2b-instruct-fp16**


- 都是支持中文
- 速度 30 tokens/s +
- 之所以没有选择limma，是因为它对中文支持的不太好

## 第二级

**codegeex4:9b-all-q5_0**  
**codegemma:7b-instruct-v1.1-q4_1**  
**deepseek-v2:16b-lite-chat-q4_0**  
**mistral-nemo:12b-instruct-2407-q4_0**  
**gemma2:9b-instruct-q4_0**

- 特殊作用：比如代码
- 速度 15 tokens/s 以上
- 运行这些模型，相比较来说，没有第一级流畅。

## 第三级

**qwen2:1.5b-instruct-fp16**  
**codegemma:2b-code-v1.1-fp16**  
**tinyllama:1.1b-chat-v1-fp16**  
**phi3.5:3.8b-mini-instruct-q8_0**

- 4b以下的模型
- 量化参数在8bit以上
- 速度在30 tokens/s 以上